{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from attack.utils import load_all_csvs\n",
    "import numpy as np\n",
    "\n",
    "def print_attack_stats(df):\n",
    "    \"\"\"\n",
    "    Splits the given DataFrame into separate attacks using separate_attacks(df),\n",
    "    and prints summary statistics for the resulting list of DataFrames.\n",
    "    \n",
    "    Statistics include:\n",
    "      - Mean, median, min, max, and standard deviation of the number of rows per attack.\n",
    "      - Number of attacks with exactly 102 rows.\n",
    "      - Indices of attacks that do not have exactly 102 rows.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to process.\n",
    "    \"\"\"\n",
    "    # Split the DataFrame into a list of attack DataFrames\n",
    "    attacks = separate_attacks(df)\n",
    "    \n",
    "    # Get the length (number of rows) for each attack\n",
    "    lengths = [len(attack) for attack in attacks]\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    mean_length   = np.mean(lengths)\n",
    "    median_length = np.median(lengths)\n",
    "    min_length    = np.min(lengths)\n",
    "    max_length    = np.max(lengths)\n",
    "    std_length    = np.std(lengths, ddof=1)  # Sample standard deviation\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Summary statistics of DataFrame lengths in 'attacks':\")\n",
    "    print(f\"\\nNumber of attacks: {len(attacks)}\")\n",
    "    print(f\"Mean:   {mean_length}\")\n",
    "    print(f\"Median: {median_length}\")\n",
    "    print(f\"Min:    {min_length}\")\n",
    "    print(f\"Max:    {max_length}\")\n",
    "    print(f\"Std:    {std_length}\")\n",
    "\n",
    "watermark_types = [\"Adaptive\", \"KGW\", \"SIR\", \"GPT4o_unwatermarked\"]\n",
    "mutators = [\n",
    "    \"DocumentMutator\", \"Document1StepMutator\", \"Document2StepMutator\",\n",
    "    \"SentenceMutator\", \"SpanMutator\", \"WordMutator\", \"EntropyWordMutator\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "def fix_encoding(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # First, try fixing common mis-encoding issues (mojibake)\n",
    "            return text.encode('latin1').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            try:\n",
    "                # If that fails, try Windows-1252 (often mixed with Latin-1)\n",
    "                return text.encode('cp1252').decode('utf-8')\n",
    "            except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "                # Return original text if all decoding attempts fail\n",
    "                return text  \n",
    "    return text  # If it's not a string, return as is\n",
    "\n",
    "\n",
    "def sanity_check_quality_preservation(df):\n",
    "    \"\"\"\n",
    "    Perform a sanity check on the DataFrame to ensure that if the previous row has \n",
    "    `quality_preserved` as True, the `mutated_text` of the previous row matches the \n",
    "    `current_text` of the current row.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to check. It must contain the columns \n",
    "                           'quality_preserved', 'mutated_text', and 'current_text'.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the index of the first row where the sanity check fails. \n",
    "              If no rows fail, nothing is printed.\n",
    "    \"\"\"\n",
    "    # Shift the 'quality_preserved' and 'mutated_text' columns to align with the next row\n",
    "    df['prev_quality_preserved'] = df['quality_preserved'].shift(1)\n",
    "    df['prev_mutated_text'] = df['mutated_text'].shift(1)\n",
    "\n",
    "    # Perform the sanity check\n",
    "    df['sanity_check'] = (\n",
    "        (df['prev_quality_preserved'] == True) & \n",
    "        (df['prev_mutated_text'] != df['current_text'])\n",
    "    )\n",
    "\n",
    "    # Find the index of the first row where the sanity check fails\n",
    "    first_failure_index = df[df['sanity_check']].index.min()\n",
    "\n",
    "    # Print the result if a failure is found\n",
    "    if not pd.isna(first_failure_index):\n",
    "        print(f\"Sanity check first failed at row index: {first_failure_index}\")\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    df.drop(columns=['prev_quality_preserved', 'prev_mutated_text', 'sanity_check'], inplace=True)\n",
    "\n",
    "def assign_unique_group_ids(df):\n",
    "    df['new_group'] = (df['step_num'] == -1).astype(int)\n",
    "    df['group_id'] = df['new_group'].cumsum()\n",
    "    return df\n",
    "\n",
    "def separate_attacks(df):\n",
    "    attacks = []\n",
    "    current_attack = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Start a new attack if the step_num resets\n",
    "        if idx > 0 and row['step_num'] < df.loc[idx - 1, 'step_num']:\n",
    "            attacks.append(pd.DataFrame(current_attack))\n",
    "            current_attack = []\n",
    "        \n",
    "        current_attack.append(row)\n",
    "    \n",
    "    # Append the last attack\n",
    "    if current_attack:\n",
    "        attacks.append(pd.DataFrame(current_attack))\n",
    "    \n",
    "    return attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dfs(df1, df2):\n",
    "    # Step 1: Create a new column selecting mutated_text when available, otherwise current_text\n",
    "    df1['selected_text'] = df1.apply(lambda row: row['mutated_text'] if pd.notna(row['mutated_text']) else row['current_text'], axis=1)\n",
    "    \n",
    "    # Step 2: Extract values where 'step_num' == -1 and strip whitespace\n",
    "    df1_filtered = df1[df1['step_num'] == -1]['selected_text'].str.strip()\n",
    "    \n",
    "    # Step 3: Strip whitespace from df2's 'text' column and count occurrences\n",
    "    match_counts = df2['text'].str.strip().apply(lambda x: (df1_filtered == x).sum())\n",
    "    \n",
    "    # Step 4: Create a result DataFrame\n",
    "    result_df = pd.DataFrame({'text': df2['text'], 'match_count': match_counts})\n",
    "    \n",
    "    # Find indices of rows in df2 that don't have a match\n",
    "    # no_match_indices = result_df[result_df['match_count'] == 0].index.tolist()\n",
    "    \n",
    "    # Print indices of unmatched rows\n",
    "    # print(f\"Indices of rows in df2 with no match: {no_match_indices}\")\n",
    "\n",
    "    # if no_match_indices:\n",
    "    #     first_no_match_index = no_match_indices[0]\n",
    "    #     first_no_match_prompt = df2.loc[first_no_match_index, 'prompt'] if 'prompt' in df2.columns else \"N/A\"\n",
    "    #     first_no_match_text = df2.loc[first_no_match_index, 'text']\n",
    "        \n",
    "    #     print(f\"First unmatched row details - Index: {first_no_match_index}\")\n",
    "    #     print(f\"Prompt: {first_no_match_prompt}\")\n",
    "    #     print(f\"Text: {first_no_match_text}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def check_step_num_validity(df):\n",
    "    if 'step_num' in df.columns:\n",
    "        step_nums = df['step_num'].values  # Extract step_num column as a NumPy array for efficient computation\n",
    "        \n",
    "        # Check condition: step_num should be either -1 or one more than the previous row\n",
    "        valid = (step_nums[0] == -1) and all(\n",
    "            (step_nums[i] == -1) or (step_nums[i] == step_nums[i - 1] + 1) for i in range(1, len(step_nums))\n",
    "        )\n",
    "\n",
    "        if not valid:\n",
    "            print(\"Warning: step_num sequence is not valid. It should either be -1 or increment by 1.\")\n",
    "\n",
    "def interpret_results(result_df):\n",
    "    # Count the number of values where match_count is not 1\n",
    "    count_not_one = (result_df['match_count'] != 1).sum()\n",
    "    count_greater_than_one = (result_df['match_count'] > 1).sum()\n",
    "    total_sum = result_df['match_count'].sum()\n",
    "\n",
    "    # Only print if values deviate from expected ones\n",
    "    if count_not_one != 0 or count_greater_than_one != 0 or total_sum != 90:\n",
    "        print(f\"Number of values where match_count is not 1: {count_not_one}\")\n",
    "        print(f\"Number of rows where match_count is greater than 1: {count_greater_than_one}\")\n",
    "        print(f\"Total sum of match_count column: {total_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing watermark_type: Adaptive, mutator: DocumentMutator\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m load_all_csvs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./attack/traces/annotated\u001b[39m\u001b[38;5;124m\"\u001b[39m, watermark_type, mutator)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./attack/traces\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatermark_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapplymap(fix_encoding)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# sanity_check_quality_preservation(df)\u001b[39;00m\n",
      "File \u001b[0;32m/data2/borito1907/sandcastles/attack/utils.py:338\u001b[0m, in \u001b[0;36mload_all_csvs\u001b[0;34m(base_dir, watermark_str, mutator_str, oracle_str, ignore_long)\u001b[0m\n\u001b[1;32m    336\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_path \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m--> 338\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Concatenate into one DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/io/parsers/readers.py:488\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1047\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1046\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m-> 1047\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col_dict:\n\u001b[1;32m   1051\u001b[0m             \u001b[38;5;66;03m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:224\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 224\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:801\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:857\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1925\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "for watermark_type in watermark_types:\n",
    "    for mutator in mutators:\n",
    "        print(f\"Processing watermark_type: {watermark_type}, mutator: {mutator}\")\n",
    "        # Load data with fallback to non-annotated directory\n",
    "        df = load_all_csvs(\"./attack/traces/annotated\", watermark_type, mutator)\n",
    "        if df.empty:\n",
    "            df = load_all_csvs(\"./attack/traces\", watermark_type, mutator)\n",
    "\n",
    "        df = df.applymap(fix_encoding)\n",
    "\n",
    "        # sanity_check_quality_preservation(df)\n",
    "\n",
    "        if 'step_num' in df.columns:\n",
    "            step_num_neg1_count = (df['step_num'] == -1).sum()\n",
    "            if step_num_neg1_count != 90:\n",
    "                print(f\"Number of rows with step_num == -1: {step_num_neg1_count}\")\n",
    "\n",
    "        check_step_num_validity(df)\n",
    "\n",
    "        entropy_df = pd.read_csv(f'/data2/borito1907/sandcastles/data/texts/entropy_control_{watermark_type}.csv')\n",
    "\n",
    "        result_df = compare_dfs(df, entropy_df)\n",
    "\n",
    "        interpret_results(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
