{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def combine_files_by_base_name(directory):\n",
    "    \"\"\"\n",
    "    Combines files in a directory based on their base name, excluding certain files.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing files to combine.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are base names and values are combined DataFrames.\n",
    "    \"\"\"\n",
    "    file_groups = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if \"Simple\" not in filename:\n",
    "            continue\n",
    "        base_name = '_'.join(filename.split('_part')[0].split('_')[:-1])\n",
    "        if base_name not in file_groups:\n",
    "            file_groups[base_name] = []\n",
    "        file_groups[base_name].append(filename)\n",
    "\n",
    "    combined_dfs = {}\n",
    "    for base_name, files in file_groups.items():\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for filename in sorted(files):  # Ensure files are processed in order of parts\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        combined_dfs[base_name] = combined_df\n",
    "\n",
    "    return combined_dfs\n",
    "\n",
    "# Helper function to separate attacks based on step_num reset\n",
    "def separate_attacks(df, length=10000):\n",
    "    attacks = []\n",
    "    current_attack = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Start a new attack if the step_num resets\n",
    "        if idx > 0 and row['Num'] < df.loc[idx - 1, 'Num']:\n",
    "            attacks.append(pd.DataFrame(current_attack))\n",
    "            current_attack = []        \n",
    "\n",
    "        current_attack.append(row)\n",
    "    \n",
    "    # Append the last attack\n",
    "    if current_attack:\n",
    "        attacks.append(pd.DataFrame(current_attack))\n",
    "    \n",
    "    return attacks\n",
    "\n",
    "def add_entropy_column(dfs):\n",
    "    \"\"\"\n",
    "    Takes a list of DataFrames, checks if the length of the list is a multiple of 10,\n",
    "    and adds an 'entropy' column to each DataFrame, containing the remainder modulo 10 of the DataFrame's position in the list.\n",
    "\n",
    "    Parameters:\n",
    "        dfs (list of pd.DataFrame): List of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: The updated list of DataFrames with the added 'entropy' column.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the length of the list is not a multiple of 10.\n",
    "    \"\"\"\n",
    "    if len(dfs) % 10 != 0:\n",
    "        raise ValueError(\"The length of the list must be a multiple of 10.\")\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        df['entropy'] = i % 10\n",
    "\n",
    "    return dfs\n",
    "\n",
    "def process_and_add_entropy(file_path, separate_attacks, add_entropy_column):\n",
    "    \"\"\"\n",
    "    Streamlines the processing of a CSV file: reads it, extracts domain from filename,\n",
    "    adds a domain column, separates attacks, and adds an entropy column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        separate_attacks (function): Function to separate attacks from the DataFrame.\n",
    "        add_entropy_column (function): Function to add an entropy column to the list of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: List of processed DataFrames with entropy column added.\n",
    "    \"\"\"\n",
    "    domain = os.path.basename(file_path).split('_')[-2]\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['domain'] = domain\n",
    "    attacks = separate_attacks(df)\n",
    "    return add_entropy_column(attacks)\n",
    "\n",
    "def check_unique_values(df, columns):\n",
    "    \"\"\"\n",
    "    Checks whether specified columns in a DataFrame have a single unique value and extracts this value.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to check.\n",
    "        columns (list of str): List of column names to check.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with column names as keys and their unique value if single, else None.\n",
    "    \"\"\"\n",
    "    unique_values = {}\n",
    "    for col in columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        if len(unique_vals) == 1:\n",
    "            unique_values[col] = unique_vals[0]\n",
    "        else:\n",
    "            unique_values[col] = None\n",
    "    return unique_values\n",
    "\n",
    "def return_correct_flipped_correct(df, prefix=\"\"):\n",
    "    df.loc[:, f'{prefix}correct'] = (df['Origin'] == df[f'{prefix}choice']).astype(float)\n",
    "    df.loc[:, f'{prefix}flipped_correct'] = (df['Origin'] == df[f'{prefix}flipped_choice']).astype(float)\n",
    "    df.loc[:, f'{prefix}avg_correct'] = (df[f'{prefix}correct'] + df[f'{prefix}flipped_correct']) / 2\n",
    "    return df[f'{prefix}correct'].mean(), df[f'{prefix}flipped_correct'].mean(),df[f'{prefix}avg_correct']. mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "# file_paths = [\n",
    "#     '/data2/borito1907/sandcastles/distinguisher/results/long_InternLMOracle_GPT4o_unwatermarked_SentenceMutator_news_SimpleDistinguisher.csv',\n",
    "#     '/data2/borito1907/sandcastles/distinguisher/results/long_InternLMOracle_GPT4o_unwatermarked_SentenceMutator_paris_SimpleDistinguisher.csv',\n",
    "#     '/data2/borito1907/sandcastles/distinguisher/results/long_InternLMOracle_GPT4o_unwatermarked_SentenceMutator_space_SimpleDistinguisher.csv'.\n",
    "# ]\n",
    "\n",
    "dir = '/data2/borito1907/sandcastles/distinguisher/results'\n",
    "\n",
    "# file_paths = [os.path.join(dir, f) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
    "# file_paths = [file for file in file_paths if \"Simple\" in file]\n",
    "\n",
    "\n",
    "combined_dataframes = combine_files_by_base_name(dir)\n",
    "\n",
    "# Process each combined DataFrame\n",
    "all_attacks = []\n",
    "all_num_attacks = []\n",
    "for base_name, combined_df in combined_dataframes.items():\n",
    "    combined_df['domain'] = base_name.split('_')[-1]  # Infer domain from base name\n",
    "    attacks = separate_attacks(combined_df)\n",
    "    attacks = add_entropy_column(attacks)\n",
    "\n",
    "    num_attacks = [attack[attack['Num'] >= 100] for attack in attacks]\n",
    "\n",
    "    all_attacks.extend(attacks)\n",
    "    all_num_attacks.extend(num_attacks)\n",
    "\n",
    "\n",
    "# Combine original DataFrames for concatenated view (optional)\n",
    "long = pd.concat(all_attacks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/borito1907/anaconda3/envs/gptq/lib/python3.10/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for attack in all_num_attacks:    \n",
    "    correct, flipped_correct, avg_correct = return_correct_flipped_correct(attack)\n",
    "    d = check_unique_values(attack, ['domain', 'entropy'])\n",
    "\n",
    "    data.append({\n",
    "        'domain' : d['domain'],\n",
    "        'entropy' : d['entropy'],\n",
    "        'correct': correct,\n",
    "        'flipped_correct': flipped_correct,\n",
    "        'avg_correct': avg_correct\n",
    "    })\n",
    "    \n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  domain   correct  flipped_correct  avg_correct\n",
      "0   news  0.880540         0.894260     0.887400\n",
      "1  paris  0.889791         0.883439     0.886615\n",
      "2  space  0.838374         0.838388     0.838381\n"
     ]
    }
   ],
   "source": [
    "grouped_df = (\n",
    "    data_df.groupby('domain')\n",
    "      .agg({\n",
    "          'correct': 'mean',\n",
    "          'flipped_correct': 'mean',\n",
    "          'avg_correct': 'mean'\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   entropy   correct  flipped_correct  avg_correct\n",
      "0      0.0  0.846510         0.855210     0.850860\n",
      "1      1.0  0.893931         0.866763     0.880347\n",
      "2      2.0  0.891955         0.874077     0.883016\n",
      "3      3.0  0.876027         0.867196     0.871612\n",
      "4      4.0  0.852963         0.899648     0.876306\n",
      "5      5.0  0.890632         0.876979     0.883805\n",
      "6      6.0  0.844089         0.842455     0.843272\n",
      "7      7.0  0.892383         0.903987     0.898185\n",
      "8      8.0  0.865311         0.864449     0.864880\n",
      "9      9.0  0.832504         0.863010     0.847757\n"
     ]
    }
   ],
   "source": [
    "grouped_df = (\n",
    "    data_df.groupby('entropy')\n",
    "      .agg({\n",
    "          'correct': 'mean',\n",
    "          'flipped_correct': 'mean',\n",
    "          'avg_correct': 'mean'\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the attacks and their corresponding avg values\n",
    "attack_avg_list = []\n",
    "\n",
    "for attack in all_attacks:\n",
    "    correct, flipped, avg = return_correct_flipped_correct(attack)\n",
    "    attack_avg_list.append((attack, avg))  # Append the attack and its avg value as a tuple\n",
    "\n",
    "# Sort the list by the avg value (second element of the tuple)\n",
    "sorted_attacks = sorted(attack_avg_list, key=lambda x: x[1])\n",
    "\n",
    "# Extract the sorted attacks and their averages separately if needed\n",
    "sorted_attacks_list = [item[0] for item in sorted_attacks]\n",
    "sorted_avgs = [item[1] for item in sorted_attacks]\n",
    "\n",
    "# # Optionally, print the sorted averages for verification\n",
    "# for attack, avg in sorted_attacks:\n",
    "#     print(f\"Attack: {attack}, Avg: {avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6393c3405f42c7a5be408e90693e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='attack_index', max=1259), Output()), _dom_classes=('wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_cumulative_avg(attack_index)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "# Function to plot cumulative avg progression for a given attack index\n",
    "def plot_cumulative_avg(attack_index):\n",
    "    # Get the attack for the given index\n",
    "    attack = sorted_attacks_list[attack_index]\n",
    "\n",
    "    avg_values = []\n",
    "    \n",
    "    # Iterate over the rows of the DataFrame cumulatively\n",
    "    for i in range(1, len(attack) + 1):\n",
    "        # Select the first `i` rows\n",
    "        subset = attack.iloc[:i]\n",
    "        \n",
    "        # Apply `return_correct_flipped_correct` to the subset\n",
    "        correct, flipped, avg = return_correct_flipped_correct(subset)\n",
    "        \n",
    "        # Store the avg value\n",
    "        avg_values.append(avg)\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(avg_values) + 1), avg_values, marker='o')\n",
    "    plt.title(f'Cumulative Avg Progression for Attack Index {attack_index}')\n",
    "    plt.xlabel('Number of Rows')\n",
    "    plt.ylabel('Average')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create an interactive slider to select attack index\n",
    "interact(\n",
    "    plot_cumulative_avg,\n",
    "    attack_index=IntSlider(min=0, max=len(sorted_attacks_list) - 1, step=1, value=100)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
